{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "Video_url = \"https://www.aznude.com/azncdn/1b0cbacd43f74817a8eddff03eb5f206/1b0cbacd43f74817a8eddff03eb5f206-hd.html\"\n",
    "\n",
    "def get_media_file_link(video_url):\n",
    "    response = requests.get(video_url)\n",
    "    video_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_video_tag_list = []\n",
    "    tag_error = \"\"\n",
    "    link_error = \"\"\n",
    "\n",
    "    try:\n",
    "        actress_video_h2_element = video_soup.find_all('h2', class_=\"video-tags\")[0]\n",
    "        actress_video_tag_element_list = actress_video_h2_element.find_all('a')\n",
    "        for tag in actress_video_tag_element_list:\n",
    "            actress_video_tag_list.append({\n",
    "                \"name\": tag.text.strip(),\n",
    "                \"link\": tag.get('href')\n",
    "            })\n",
    "    except Exception as e:\n",
    "        tag_error = str(e)\n",
    "\n",
    "    download_link = \"\"\n",
    "\n",
    "    try:\n",
    "        download_div = video_soup.find_all('div', class_='videoButtons')\n",
    "        for div in download_div:\n",
    "            if div.text == 'Download':\n",
    "                download_link = \"https:\" + div.parent.get('href')\n",
    "    except Exception as e:\n",
    "        link_error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"tag_list\": actress_video_tag_list,\n",
    "        \"media_file_link\": download_link,\n",
    "        \"tag_error\": tag_error,\n",
    "        \"link_error\": link_error\n",
    "    }\n",
    "\n",
    "get_media_file_link(video_url=Video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    try:\n",
    "        birthplace = soup.find(\n",
    "            'div', class_='banner-info').find('a').text.strip()\n",
    "    except:\n",
    "        birthplace = \"\"\n",
    "    # birthplace = soup.find('div', class_='banner-info').find('a').text.strip()\n",
    "    hashtag = soup.find(\n",
    "        'span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    celeb_img_url = soup.find(\n",
    "        'img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "def process_page_index(index):\n",
    "    print(\"actress page index: \", index)\n",
    "    popular_actress_url = f'https://www.aznude.com/browse/celebs/popular/{\n",
    "        index}.html'\n",
    "    response = requests.get(popular_actress_url)\n",
    "    soup_popular_actress = BeautifulSoup(response.text, 'html.parser')\n",
    "    actress_div_list = soup_popular_actress.find_all(\n",
    "        'div', class_=\"col-lg-2 col-md-3 col-sm-4 col-xs-6 story-thumbs celebs-boxes\")\n",
    "    actress_portfolio_list = []\n",
    "    failed_actress_portfolio_list = []\n",
    "\n",
    "    in_page_index = 0\n",
    "\n",
    "    for actress in actress_div_list:\n",
    "        actress_url = actress.find('a').get('href')\n",
    "        actress_name = actress.find('h4').text.strip()\n",
    "        try:\n",
    "            view_count = actress.find('span').text.strip()\n",
    "            actress_portfolio = get_actress_portfolio(\n",
    "                url=\"https://aznude.com\" + actress_url, view_count=view_count)\n",
    "            actress_portfolio_list.append(actress_portfolio)\n",
    "            print(actress_name)\n",
    "        except Exception as e:\n",
    "            failed_stat = {\n",
    "                \"actress_url\": \"https://aznude.com\" + actress_url,\n",
    "                \"actress_name\": actress_name,\n",
    "                \"in_page_index\": in_page_index,\n",
    "                \"page_index\": index,\n",
    "                \"view_count\": view_count,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            # print(failed_stat)\n",
    "            failed_actress_portfolio_list.append(failed_stat)\n",
    "            failed_json_string = json.dumps(failed_stat)\n",
    "            with open(f'.\\\\failed_actress_list\\\\failed_{index}_{actress_name}.json', 'w') as failed_json_file:\n",
    "                failed_json_file.write(failed_json_string)\n",
    "            print(actress_name, \"failed\")\n",
    "        in_page_index += 1\n",
    "\n",
    "    json_string = json.dumps(actress_portfolio_list)\n",
    "    with open(f'.\\\\actress_portfolio\\\\actress_portfolio_list_{index}.json', 'w') as json_file:\n",
    "        json_file.write(json_string)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_dotenv()\n",
    "    start_index = int(os.getenv(\"START_INDEX\"))\n",
    "    end_index = int(os.getenv(\"END_INDEX\"))\n",
    "    num_processes = int(os.getenv(\"NUMBER_OF_PROCESSES\"))\n",
    "\n",
    "    scraping_starting_time = datetime.datetime.now()\n",
    "\n",
    "    with Pool(num_processes) as p:\n",
    "        p.map(process_page_index, range(start_index, end_index + 1))\n",
    "\n",
    "    scraping_done_time = datetime.datetime.now()\n",
    "    print(f\"Starting Scraping from page {start_index} to {end_index} at {\n",
    "          scraping_starting_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "    print(f\"Completed Scraping from page {start_index} to {end_index} at {\n",
    "          scraping_done_time.strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "\n",
    "    print(\"Elapsed Time: \", scraping_done_time - scraping_starting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_index = 51\n",
    "end_index = 100\n",
    "\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    try:\n",
    "        birthplace = soup.find(\n",
    "            'div', class_='banner-info').find('a').text.strip()\n",
    "    except:\n",
    "        birthplace = \"\"\n",
    "    # birthplace = soup.find('div', class_='banner-info').find('a').text.strip()\n",
    "    hashtag = soup.find(\n",
    "        'span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    celeb_img_url = soup.find(\n",
    "        'img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "for index in range(start_index, end_index+1):\n",
    "    popular_actress_url = f'https://www.aznude.com/browse/celebs/popular/{\n",
    "        index}.html'\n",
    "    response = requests.get(popular_actress_url)\n",
    "    soup_popular_actress = BeautifulSoup(response.text, 'html.parser')\n",
    "    actress_div_list = soup_popular_actress.find_all(\n",
    "        'div', class_=\"col-lg-2 col-md-3 col-sm-4 col-xs-6 story-thumbs celebs-boxes\")\n",
    "    actress_portfolio_list = []\n",
    "    failed_actress_portfolio_list = []\n",
    "\n",
    "    in_page_index = 0\n",
    "\n",
    "    for actress in actress_div_list:\n",
    "        actress_url = actress.find('a').get('href')\n",
    "        actress_name = actress.find('h4').text.strip()\n",
    "        try:\n",
    "            view_count = actress.find('span').text.strip()\n",
    "            actress_portfolio = get_actress_portfolio(\n",
    "                url=\"https://aznude.com\" + actress_url, view_count=view_count)\n",
    "            actress_portfolio_list.append(actress_portfolio)\n",
    "            print(actress_name)\n",
    "        except:\n",
    "            failed_stat = {\n",
    "                \"actress_url\": \"https://aznude.com\" + actress_url,\n",
    "                \"actress_name\": actress_name,\n",
    "                \"in_page_index\": in_page_index,\n",
    "                \"page_index\": index,\n",
    "                \"view_count\": view_count,\n",
    "            }\n",
    "            print(failed_stat)\n",
    "            failed_actress_portfolio_list.append(failed_stat)\n",
    "            failed_json_string = json.dumps(failed_stat)\n",
    "            with open(f'.\\\\failed_actress_list\\\\failed_{index}_{actress_name}.json', 'w') as failed_json_file:\n",
    "                failed_json_file.write(failed_json_string)\n",
    "            print(actress_name, \"failed\")\n",
    "        in_page_index += 1\n",
    "\n",
    "    json_string = json.dumps(actress_portfolio_list)\n",
    "    with open(f'.\\\\actress_portfolio\\\\actress_portfolio_list_{index}.json', 'w') as json_file:\n",
    "        json_file.write(json_string)\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
