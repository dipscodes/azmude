{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing import Pool\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def complete_links(link):\n",
    "    if (link.startswith(\"/\") and not link.startswith(\"//\")):\n",
    "        return \"https://www.aznude.com\" + link\n",
    "\n",
    "    elif (link.startswith(\"//\")):\n",
    "        return \"https:\" + link\n",
    "\n",
    "    elif link.startswith(\"https\") or link.startswith(\"http\"):\n",
    "        return link\n",
    "\n",
    "\n",
    "def index_to_file_path(index):\n",
    "    return os.path.join('.', 'updated_pages', f'actress_portfolio_list_{index}.json')\n",
    "\n",
    "\n",
    "def index_to_save_file_path(index):\n",
    "    return os.path.join('.', 'updated_pages', f'actress_portfolio_list_{index}.json')\n",
    "\n",
    "\n",
    "def get_media_file_link_and_tags(video_url):\n",
    "    response = requests.get(video_url)\n",
    "    video_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_video_tag_list = []\n",
    "    tag_error = \"\"\n",
    "    link_error = \"\"\n",
    "\n",
    "    try:\n",
    "        actress_video_h2_element = video_soup.find_all(\n",
    "            'h2', class_=\"video-tags\")[0]\n",
    "        actress_video_tag_element_list = actress_video_h2_element.find_all('a')\n",
    "        for tag in actress_video_tag_element_list:\n",
    "            actress_video_tag_list.append({\n",
    "                \"name\": tag.text.strip(),\n",
    "                \"link\": complete_links(tag.get('href'))\n",
    "            })\n",
    "    except Exception as e:\n",
    "        tag_error = str(e)\n",
    "\n",
    "    download_link = \"\"\n",
    "\n",
    "    try:\n",
    "        download_div = video_soup.find_all('div', class_='videoButtons')\n",
    "        for div in download_div:\n",
    "            if div.text == 'Download':\n",
    "                download_link = complete_links(div.parent.get('href'))\n",
    "    except Exception as e:\n",
    "        link_error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"tag_list\": actress_video_tag_list,\n",
    "        \"media_file_link\": download_link,\n",
    "        \"tag_error\": tag_error,\n",
    "        \"link_error\": link_error\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_actress_portfolio_file():\n",
    "    folder_path = os.path.join('.', 'actress_portfolio')\n",
    "    try:\n",
    "        files = os.listdir(folder_path)\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The folder '{folder_path}' does not exist.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and returns its content as a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "        None: If the file cannot be opened or the JSON data is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error reading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_meadia_file_link_and_tags_to_page(index):\n",
    "    portfolio_page_file_path = index_to_file_path(index)\n",
    "    page_content = read_json_file(portfolio_page_file_path)\n",
    "    for actress in page_content:\n",
    "        print(actress[\"actress_name\"])\n",
    "        production_media_list = actress[\"production_media_list\"]\n",
    "        for production in production_media_list:\n",
    "            print(\"    \" + production[\"title\"])\n",
    "            media_list = production[\"media_list\"]\n",
    "            for media in media_list:\n",
    "                if media[\"type\"] == \"video\":\n",
    "                    link_to_media = complete_links(media[\"link_to_media\"])\n",
    "                    eid = complete_links(media[\"eid\"])\n",
    "                    image_source = complete_links(media[\"image_source\"])\n",
    "                    # link_and_tags = get_media_file_link_and_tags(video_url=link_to_media)\n",
    "\n",
    "                    media[\"link_to_media\"] = link_to_media\n",
    "                    media[\"eid\"] = eid\n",
    "                    media[\"image_source\"] = image_source\n",
    "                    tag_list = media[\"tag_list\"]\n",
    "                    for tag in tag_list:\n",
    "                        tag[\"link\"] = complete_links(tag[\"link\"])\n",
    "                    # media[\"tag_list\"] = link_and_tags[\"tag_list\"]\n",
    "                    # media[\"media_file_link\"] = link_and_tags[\"media_file_link\"]\n",
    "                    # media[\"tag_error\"] = link_and_tags[\"tag_error\"]\n",
    "                    # media[\"link_error\"] = link_and_tags[\"link_error\"]\n",
    "            # break\n",
    "        # break\n",
    "    return page_content\n",
    "\n",
    "# page_content = add_meadia_file_link_and_tags_to_page(1)\n",
    "\n",
    "def save_updated_page(index):\n",
    "    page_content = add_meadia_file_link_and_tags_to_page(index)\n",
    "    json_string = json.dumps(page_content)\n",
    "    with open(index_to_save_file_path(index), 'w') as json_file:\n",
    "        json_file.write(json_string)\n",
    "\n",
    "\n",
    "save_updated_page(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def get_media_file_link_and_tags(video_url):\n",
    "    response = requests.get(video_url)\n",
    "    video_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_video_tag_list = []\n",
    "    tag_error = \"\"\n",
    "    link_error = \"\"\n",
    "\n",
    "    try:\n",
    "        actress_video_h2_element = video_soup.find_all(\n",
    "            'h2', class_=\"video-tags\")[0]\n",
    "        actress_video_tag_element_list = actress_video_h2_element.find_all('a')\n",
    "        for tag in actress_video_tag_element_list:\n",
    "            actress_video_tag_list.append({\n",
    "                \"name\": tag.text.strip(),\n",
    "                \"link\": tag.get('href')\n",
    "            })\n",
    "    except Exception as e:\n",
    "        tag_error = str(e)\n",
    "\n",
    "    download_link = \"\"\n",
    "\n",
    "    try:\n",
    "        download_div = video_soup.find_all('div', class_='videoButtons')\n",
    "        for div in download_div:\n",
    "            if div.text == 'Download':\n",
    "                download_link = \"https:\" + div.parent.get('href')\n",
    "    except Exception as e:\n",
    "        link_error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"tag_list\": actress_video_tag_list,\n",
    "        \"media_file_link\": download_link,\n",
    "        \"tag_error\": tag_error,\n",
    "        \"link_error\": link_error\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_actress_portfolio_file():\n",
    "    folder_path = os.path.join('.', 'actress_portfolio')\n",
    "    try:\n",
    "        files = os.listdir(folder_path)\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The folder '{folder_path}' does not exist.\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and returns its content as a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "        None: If the file cannot be opened or the JSON data is invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error reading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_meadia_file_link_and_tags_to_page(portfolio_page_file_path):\n",
    "    page_content = read_json_file(portfolio_page_file_path)\n",
    "    for actress in page_content:\n",
    "        print(actress[\"actress_name\"])\n",
    "        production_media_list = actress[\"production_media_list\"]\n",
    "        for production in production_media_list:\n",
    "            print(\"    \" + production[\"title\"])\n",
    "            media_list = production[\"media_list\"]\n",
    "            for media in media_list:\n",
    "                if media[\"type\"] == \"video\":\n",
    "                    link_to_media = \"https://www.aznude.com\" + media[\"link_to_media\"]\n",
    "                    # eid = \"https://www.aznude.com\" + media[\"eid\"]\n",
    "                    # image_source = \"https:\" + media[\"image_source\"]\n",
    "                    link_and_tags = get_media_file_link_and_tags(video_url=link_to_media)\n",
    "\n",
    "                    # media[\"link_to_media\"] = link_to_media\n",
    "                    # media[\"eid\"] = eid\n",
    "                    # media[\"image_source\"] = image_source\n",
    "                    media[\"tag_list\"] = link_and_tags[\"tag_list\"]\n",
    "                    media[\"media_file_link\"] = link_and_tags[\"media_file_link\"]\n",
    "                    media[\"tag_error\"] = link_and_tags[\"tag_error\"]\n",
    "                    media[\"link_error\"] = link_and_tags[\"link_error\"]\n",
    "                    # print(media[\"link_to_media\"])\n",
    "        break\n",
    "    return page_content\n",
    "\n",
    "\n",
    "first_page_content = add_meadia_file_link_and_tags_to_page(os.path.join(\n",
    "    '.', 'actress_portfolio', 'actress_portfolio_list_2510.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_links(link):\n",
    "    if (link.startswith(\"/\") and not link.startswith(\"//\")):\n",
    "        return \"https://www.aznude.com\" + link\n",
    "\n",
    "    elif (link.startswith(\"//\")):\n",
    "        return \"https:\" + link\n",
    "\n",
    "    elif link.startswith(\"https\") or link.startswith(\"http\"):\n",
    "        return link\n",
    "    \n",
    "\n",
    "complete_links(\n",
    "    \"https://cdn2.aznude.com/44c150c0fc144b53b09121bea062326f/44c150c0fc144b53b09121bea062326f.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    try:\n",
    "        birthplace = soup.find(\n",
    "            'div', class_='banner-info').find('a').text.strip()\n",
    "    except:\n",
    "        birthplace = \"\"\n",
    "    # birthplace = soup.find('div', class_='banner-info').find('a').text.strip()\n",
    "    hashtag = soup.find(\n",
    "        'span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    celeb_img_url = soup.find(\n",
    "        'img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "def process_page_index(index):\n",
    "    print(\"actress page index: \", index)\n",
    "    # popular_actress_url = f'https://www.aznude.com/browse/celebs/popular/{\n",
    "    #     index}.html'\n",
    "    # response = requests.get(popular_actress_url)\n",
    "    # soup_popular_actress = BeautifulSoup(response.text, 'html.parser')\n",
    "    # actress_div_list = soup_popular_actress.find_all(\n",
    "    #     'div', class_=\"col-lg-2 col-md-3 col-sm-4 col-xs-6 story-thumbs celebs-boxes\")\n",
    "    # actress_portfolio_list = []\n",
    "    # failed_actress_portfolio_list = []\n",
    "\n",
    "    # in_page_index = 0\n",
    "\n",
    "    # for actress in actress_div_list:\n",
    "    #     actress_url = actress.find('a').get('href')\n",
    "    #     actress_name = actress.find('h4').text.strip()\n",
    "    #     try:\n",
    "    #         view_count = actress.find('span').text.strip()\n",
    "    #         actress_portfolio = get_actress_portfolio(\n",
    "    #             url=\"https://aznude.com\" + actress_url, view_count=view_count)\n",
    "    #         actress_portfolio_list.append(actress_portfolio)\n",
    "    #         print(actress_name)\n",
    "    #     except Exception as e:\n",
    "    #         failed_stat = {\n",
    "    #             \"actress_url\": \"https://aznude.com\" + actress_url,\n",
    "    #             \"actress_name\": actress_name,\n",
    "    #             \"in_page_index\": in_page_index,\n",
    "    #             \"page_index\": index,\n",
    "    #             \"view_count\": view_count,\n",
    "    #             \"error\": str(e)\n",
    "    #         }\n",
    "    #         # print(failed_stat)\n",
    "    #         failed_actress_portfolio_list.append(failed_stat)\n",
    "    #         failed_json_string = json.dumps(failed_stat)\n",
    "    #         with open(f'.\\\\failed_actress_list\\\\failed_{index}_{actress_name}.json', 'w') as failed_json_file:\n",
    "    #             failed_json_file.write(failed_json_string)\n",
    "    #         print(actress_name, \"failed\")\n",
    "    #     in_page_index += 1\n",
    "\n",
    "    # json_string = json.dumps(actress_portfolio_list)\n",
    "    # with open(f'.\\\\actress_portfolio\\\\actress_portfolio_list_{index}.json', 'w') as json_file:\n",
    "    #     json_file.write(json_string)\n",
    "\n",
    "    # return 0\n",
    "\n",
    "\n",
    "start_index = 51\n",
    "end_index = 60\n",
    "num_processes = 2  # You can adjust this value based on your CPU cores\n",
    "p = Pool(2)\n",
    "\n",
    "p.map(process_page_index, range(start_index, end_index + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
