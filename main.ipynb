{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_production_info(section_element):\n",
    "    h4_element = section_element.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(next_sibling):\n",
    "    if (not next_sibling):\n",
    "        return None\n",
    "    if next_sibling.find('a', class_='video'):\n",
    "        a_tag = next_sibling.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif next_sibling.find('a', class_='picture'):\n",
    "        a_tag = next_sibling.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_index = 1\n",
    "end_index = 50\n",
    "\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    try:\n",
    "        birthplace = soup.find(\n",
    "            'div', class_='banner-info').find('a').text.strip()\n",
    "    except:\n",
    "        birthplace = \"\"\n",
    "    hashtag = soup.find(\n",
    "        'span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    star_rating = soup.find('span', class_='rating-score').text.strip()\n",
    "    celeb_img_url = soup.find(\n",
    "        'img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"star_rating\": star_rating,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "# print(get_actress_portfolio('https://www.aznude.com/view/celeb/e/estheracebo.html', view_count=0))\n",
    "\n",
    "# Read the names from the text file into a list\n",
    "with open(\"actress_list.txt\", \"r\") as file:\n",
    "    names = file.readlines()\n",
    "\n",
    "# Remove any leading or trailing whitespace characters\n",
    "names = [name.strip() for name in names]\n",
    "\n",
    "names_dict = {}\n",
    "\n",
    "for index in range(len(names)):\n",
    "    if \"failed\" in names[index]:\n",
    "        key = names[index].replace(\"failed\", \"\").strip()\n",
    "        names_dict[key] = {\n",
    "            \"place\": (index % 22),\n",
    "            \"page_index\": (index // 22)\n",
    "        }\n",
    "\n",
    "folder_path = \"E:\\\\VSCode\\\\aznude\\\\actress_portfolio\"\n",
    "actr_list_path = \"E:\\\\VSCode\\\\aznude\\\\actrss\"\n",
    "\n",
    "json_contents = []\n",
    "\n",
    "json_files = [file_name for file_name in os.listdir(\n",
    "    folder_path) if file_name.endswith('.json')]\n",
    "\n",
    "\n",
    "for file_name in json_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    if os.path.isfile(file_path) and file_name.endswith('.json'):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            page_index = names_dict[data[1]][\"page_index\"]\n",
    "            place = names_dict[data[1]][\"place\"]\n",
    "            name = data[1]\n",
    "            actrs_file_path = f\"E:\\\\VSCode\\\\aznude\\\\actrss\\\\actress_portfolio_list_{page_index}.json\"\n",
    "\n",
    "            print(name, page_index+1, place)\n",
    "\n",
    "            actrs_url = \"https://aznude.com\" + data[0]\n",
    "            test = get_actress_portfolio(actrs_url)\n",
    "            json_contents.append(data)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the names from the text file into a list\n",
    "with open(\"actress_list.txt\", \"r\") as file:\n",
    "    names = file.readlines()\n",
    "\n",
    "# Remove any leading or trailing whitespace characters\n",
    "names = [name.strip() for name in names]\n",
    "\n",
    "names_dict = {}\n",
    "\n",
    "for index in range(len(names)):\n",
    "    if \"failed\" in names[index]:\n",
    "        key = names[index].replace(\"failed\", \"\").strip()\n",
    "        names_dict[key] = {\n",
    "            \"place\": (index % 22) + 1,\n",
    "            \"page_index\": (index // 22) if (index // 22) else 1\n",
    "        }\n",
    "\n",
    "names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_index = 1\n",
    "end_index = 50\n",
    "\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    try:\n",
    "        birthplace = soup.find('div', class_='banner-info').find('a').text.strip()\n",
    "    except:\n",
    "        birthplace = \"\"\n",
    "    hashtag = soup.find(\n",
    "        'span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    star_rating = soup.find('span', class_='rating-score').text.strip()\n",
    "    celeb_img_url = soup.find(\n",
    "        'img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"star_rating\": star_rating,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "anadearmas = get_actress_portfolio('https://www.aznude.com/view/celeb/a/anadearmas.html', view_count=0)\n",
    "json_string = json.dumps(anadearmas)\n",
    "with open('anadearmas.json', 'w') as json_file:\n",
    "    json_file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_index = 1\n",
    "end_index = 50\n",
    "\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    try:\n",
    "        birthplace = soup.find('div', class_='banner-info').find('a').text.strip()\n",
    "    except:\n",
    "        birthplace = \"\"\n",
    "    hashtag = soup.find('span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    celeb_img_url = soup.find('img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "for index in range(start_index, end_index+1):\n",
    "    popular_actress_url = f'https://www.aznude.com/browse/celebs/popular/{index}.html'\n",
    "    response = requests.get(popular_actress_url)\n",
    "    soup_popular_actress = BeautifulSoup(response.text, 'html.parser')\n",
    "    actress_div_list = soup_popular_actress.find_all(\n",
    "        'div', class_=\"col-lg-2 col-md-3 col-sm-4 col-xs-6 story-thumbs celebs-boxes\")\n",
    "    actress_portfolio_list = []\n",
    "    failed_actress_portfolio_list = []\n",
    "\n",
    "    for actress in actress_div_list:\n",
    "        actress_url = actress.find('a').get('href')\n",
    "        actress_name = actress.find('h4').text.strip()\n",
    "        try:\n",
    "            view_count = actress.find('span').text.strip()\n",
    "            actress_portfolio = get_actress_portfolio(\n",
    "                url=\"https://aznude.com\" + actress_url, view_count=view_count)\n",
    "            actress_portfolio_list.append(actress_portfolio)\n",
    "            print(actress_name)\n",
    "        except:\n",
    "            failed_actress_portfolio_list.append([actress_url, actress_name])\n",
    "            failed_json_string = json.dumps([actress_url, actress_name])\n",
    "            with open(f'.\\\\failed_actress_list\\\\failed_{index}_{actress_name}.json', 'w') as failed_json_file:\n",
    "                failed_json_file.write(failed_json_string)\n",
    "            print(actress_name, \"failed\")\n",
    "\n",
    "    json_string = json.dumps(actress_portfolio_list)\n",
    "    with open(f'.\\\\actress_portfolio\\\\actress_portfolio_list_{index}.json', 'w') as json_file:\n",
    "        json_file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_index = 1\n",
    "end_index = 50\n",
    "\n",
    "\n",
    "def get_production_info(production):\n",
    "    h4_element = production.find('h4')\n",
    "    type = h4_element.span.text.strip().lower().replace(\":\", \"\").capitalize()\n",
    "    title = h4_element.a.text.strip()\n",
    "    release_info = h4_element.contents[-1].strip().strip('()')\n",
    "\n",
    "    ongoing_pattern = re.compile(r'(\\d{4})-')  # Matches \"YYYY-\"\n",
    "    range_pattern = re.compile(r'(\\d{4})-(\\d{4})')  # Matches \"YYYY-YYYY\"\n",
    "    year_pattern = re.compile(r'^(\\d{4})$')  # Matches \"YYYY\"\n",
    "\n",
    "    ongoing_match = ongoing_pattern.search(release_info)\n",
    "    range_match = range_pattern.search(release_info)\n",
    "    year_match = year_pattern.search(release_info)\n",
    "\n",
    "    if range_match:\n",
    "        start_year = range_match.group(1)\n",
    "        end_year = range_match.group(2)\n",
    "        ongoing = False\n",
    "    elif ongoing_match:\n",
    "        start_year = ongoing_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = True\n",
    "    elif year_match:\n",
    "        start_year = year_match.group(1)\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "        ongoing = False\n",
    "\n",
    "    result = {\n",
    "        \"type\": type,\n",
    "        \"title\": title,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year,\n",
    "        \"ongoing\": ongoing,\n",
    "        \"media_list\": []\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_media_info(media):\n",
    "    if (not media):\n",
    "        return None\n",
    "    if media.find('a', class_='video'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        eid = a_tag.get('eid')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": eid,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"video\"\n",
    "        }\n",
    "        return result\n",
    "    elif media.find('a', class_='picture'):\n",
    "        a_tag = media.find('a')\n",
    "        href = a_tag.get('href')\n",
    "        img_src = a_tag.find('img')['src']\n",
    "        result = {\n",
    "            \"link_to_media\": href,\n",
    "            \"eid\": None,\n",
    "            \"image_source\": img_src,\n",
    "            \"type\": \"picture\"\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_actress_portfolio(url, view_count):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    actress_name = soup.find('h1').text.strip()\n",
    "    actress_name = re.sub(r'#.*', '', actress_name).strip()\n",
    "    # try:\n",
    "    #     birthplace = soup.find(\n",
    "    #         'div', class_='banner-info').find('a').text.strip()\n",
    "    # except:\n",
    "    #     birthplace = \"\"\n",
    "    birthplace = soup.find('div', class_='banner-info').find('a').text.strip()\n",
    "    hashtag = soup.find(\n",
    "        'span', class_='tag-desktop').text.strip().replace(\"#\", \"\")\n",
    "    celeb_img_url = soup.find(\n",
    "        'img', class_=\"img-circle pull-right img-responsive celeb-img\").get('src')\n",
    "\n",
    "    actress_portfolio = {\n",
    "        \"actress_name\": actress_name,\n",
    "        \"birthplace\": birthplace,\n",
    "        \"hashtag\": hashtag,\n",
    "        \"celeb_img_url\": celeb_img_url,\n",
    "        \"view_count\": view_count,\n",
    "        \"url\": url,\n",
    "        \"production_media_list\": []\n",
    "    }\n",
    "\n",
    "    production_media_list = []\n",
    "    production_section_list = soup.find_all('section')\n",
    "\n",
    "    for production in production_section_list:\n",
    "        media_div_for_section = production.find_next_sibling()\n",
    "        production_info = get_production_info(production)\n",
    "\n",
    "        if media_div_for_section and media_div_for_section.name == 'div':\n",
    "            media_div_list = media_div_for_section.find_all(\n",
    "                'div', class_='col-lg-3 col-sm-4 col-xs-6 celebs-boxes albuma')\n",
    "            media_list = []\n",
    "            for media in media_div_list:\n",
    "                media_info = get_media_info(media)\n",
    "                media_list.append(media_info)\n",
    "            production_info[\"media_list\"] = media_list\n",
    "\n",
    "        production_media_list.append(production_info)\n",
    "\n",
    "    actress_portfolio[\"production_media_list\"] = production_media_list\n",
    "    return actress_portfolio\n",
    "\n",
    "\n",
    "for index in range(start_index, end_index+1):\n",
    "    popular_actress_url = f'https://www.aznude.com/browse/celebs/popular/{\n",
    "        index}.html'\n",
    "    response = requests.get(popular_actress_url)\n",
    "    soup_popular_actress = BeautifulSoup(response.text, 'html.parser')\n",
    "    actress_div_list = soup_popular_actress.find_all(\n",
    "        'div', class_=\"col-lg-2 col-md-3 col-sm-4 col-xs-6 story-thumbs celebs-boxes\")\n",
    "    actress_portfolio_list = []\n",
    "    failed_actress_portfolio_list = []\n",
    "\n",
    "    in_page_index = 0\n",
    "\n",
    "    for actress in actress_div_list:\n",
    "        if in_page_index != 20:\n",
    "            in_page_index += 1\n",
    "            continue\n",
    "        actress_url = actress.find('a').get('href')\n",
    "        actress_name = actress.find('h4').text.strip()\n",
    "        try:\n",
    "            view_count = actress.find('span').text.strip()\n",
    "            actress_portfolio = get_actress_portfolio(\n",
    "                url=\"https://aznude.com\" + actress_url, view_count=view_count)\n",
    "            actress_portfolio_list.append(actress_portfolio)\n",
    "            print(actress_name)\n",
    "        except:\n",
    "            failed_stat = {\n",
    "                \"actress_url\": \"https://aznude.com\" + actress_url,\n",
    "                \"actress_name\": actress_name,\n",
    "                \"in_page_index\": in_page_index,\n",
    "                \"page_index\": index,\n",
    "                \"view_count\": view_count,\n",
    "            }\n",
    "            print(failed_stat)\n",
    "            failed_actress_portfolio_list.append(failed_stat)\n",
    "            failed_json_string = json.dumps(failed_stat)\n",
    "            with open(f'.\\\\failed_actress_list\\\\failed_{index}_{actress_name}.json', 'w') as failed_json_file:\n",
    "                failed_json_file.write(failed_json_string)\n",
    "            print(actress_name, \"failed\")\n",
    "        in_page_index += 1\n",
    "\n",
    "    json_string = json.dumps(actress_portfolio_list)\n",
    "    with open(f'.\\\\actress_portfolio\\\\actress_portfolio_list_{index}.json', 'w') as json_file:\n",
    "        json_file.write(json_string)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_1.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_10.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_11.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_12.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_13.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_14.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_15.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_16.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_17.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_18.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_19.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_2.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_20.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_21.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_22.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_23.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_24.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_25.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_26.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_27.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_28.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_29.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_3.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_30.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_31.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_32.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_33.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_34.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_35.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_36.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_37.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_38.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_39.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_4.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_40.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_41.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_42.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_43.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_44.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_45.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_46.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_47.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_48.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_49.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_5.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_50.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_6.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_7.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_8.json 22\n",
      "E:\\VSCode\\aznude\\actress_portfolio\\actress_portfolio_list_9.json 22\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "folder_path = \"E:\\\\VSCode\\\\aznude\\\\actress_portfolio\"\n",
    "\n",
    "json_files = [file_name for file_name in os.listdir(\n",
    "    folder_path) if file_name.endswith('.json')]\n",
    "\n",
    "\n",
    "for file_name in json_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    if os.path.isfile(file_path) and file_name.endswith('.json'):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            print(file_path, len(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
